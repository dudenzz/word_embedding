# coding=utf-8
import sys
import os
import time
import io

import numpy as np

from glove_simple import Glove
from gensim.models import Word2Vec

from models_params import model_params

from scipy.spatial.distance import cosine, euclidean


class VectorModelWrap:
    def __init__(self, model_name, glove=False, binary=True, dims=300, models_path="F:\\wiki"):
        """
        Konstruktor wrappera modeli wektorowych
        :param model_name: nazwa pliku modeli
        :param glove: flaga czy model jest w formacie glove czy word2vec
        :param binary: flaga czy model w2v jest w formacie binarnym czy tekstowym
        :param dims: liczba wymiarów wektorów w modelu
        """
        self._model_name = model_name
        self._model_path = os.path.join(models_path, model_name)

        self._glove = glove
        self._binary = binary
        self.dims = dims

        if self._glove:
            self._model = VectorModelWrap.load_stf(self._model_path, self.dims)
        else:
            self._model = Word2Vec.load_word2vec_format(self._model_path, binary=True) if self._binary else\
                Word2Vec.load(self._model_path)

    @staticmethod
    def load_stf(filename, no_d):
        """
        Load model from the output files generated by
        the C code from http://nlp.stanford.edu/projects/glove/.

        The entries of the word dictionary will be of type
        unicode in Python 2 and str in Python 3.
        :param filename: nazwa pliku z modelem wektorowym glove
        :param no_d: liczba wymiarów wektorów w modelu
        :return:
        """
        start = time.clock()
        dct = {}
        size = VectorModelWrap.calculate_size(filename)
        vectors = np.ndarray(shape=(size, no_d), dtype=float)
        _iter = 0
        with io.open(filename, 'r', encoding='utf-8') as savefile:
            for i, line in enumerate(savefile):
                tokens = line.split(' ')
                # if _iter % 10000 == 0:
                #    print(_iter, size, 'stamp', time.clock() - start)
                word = tokens[0]

                vectors[_iter] = tokens[1:]
                dct[word] = i
                _iter += 1

        # Infer word vectors dimensions.

        no_vectors = len(dct)
        print('stampnv', time.clock() - start)
        # Set up the model instance.
        instance = Glove()
        print('stampinst', time.clock() - start)
        instance.no_components = size
        print('stampnoc', time.clock() - start)
        instance.word_vectors = vectors
        print('stampwv', time.clock() - start)
        instance.word_biases = np.zeros(no_vectors)
        print('stampwb', time.clock() - start)
        instance.add_dictionary(dct)
        print('stampdict', time.clock() - start)
        return instance

    @staticmethod
    def calculate_size(filename):
        """
        Oblicza rozmiar słownika modelu glove w zadanym pliku.
        :param filename: path do pliku
        :return: liczba wektorów w pliku
        """
        _size = 0
        with io.open(filename, 'r', encoding='utf-8') as savefile:
            for _, _ in enumerate(savefile):
                # if _size % 100000 == 0:
                #     print(_size, 'stamp', time.clock() - start)
                _size += 1
        return _size

    def word_in_model(self, word):
        """
        Pobiera embedding vector dla danego słowa
        :param word: słowo dla którego jest poszukiwany wektor
        :return: embedding vector dla danego słowa
        """
        if self._glove:
            return word in self._model.dictionary
        else:
            return word in self._model.vocab

    def normalize_model(self):
        """
        Uwaga, dla glove nie testowane!
        Wykonuje normalizację zastosowanego modelu
        """
        if self._glove:
            norm_vector = np.sqrt(np.sum(np.power(self._model.word_vectors, 2), 1))

            self._model.syn0 = (self._model.word_vectors.T / norm_vector).T
        else:
            norm_vector = np.sqrt(np.sum(np.power(self._model.syn0, 2), 1))

            self._model.syn0 = (self._model.syn0.T / norm_vector).T

    def get_vocab_size(self):
        if self._glove:
            return len(self._model.word_vectors)
        else:
            return len(self._model.vocab)

    def get_word(self, i):
        if self._glove:
            return self._model.inverse_dictionary[i]
        else:
            return self._model.index2word[i]

    def get_word_vector(self, word):
        if self._glove:
            return self._model.word_vectors[self._model.dictionary[word]]
        else:
            return self._model[word]

    def most_similar(self, positive=None, negative=None, number=5):
        if self._glove:
            return self._model.most_similar_multiple_words(positive, negative, number)
        else:
            return self._model.most_similar(positive, negative, number)


if __name__ == '__main__':
    chosen_source = ["glove_large_1_by_12_svd.bin", "glove_large_1_by_32_svd.bin",
                     "glove_large_1_by_39_svd.bin", "glove_large_1_by_40_svd.bin", "glove_large_1_by_42_svd.bin",
                     "glove_large_1_by_5_svd.bin"]

    for m in chosen_source:
        vmw = VectorModelWrap(m, glove=model_params[m]['glove'], binary=model_params[m]['binary'],
                              dims=model_params[m]['dims'])

        print np.mean([np.sqrt(np.sum(np.power(vmw._model.syn0[i, :], 2))) for i in xrange(10000)])

    '''
    wmv_1 = VectorModelWrap("glove_large.bin", glove=model_params["glove_large.bin"]['glove'],
                            binary=model_params["glove_large.bin"]['binary'],
                            dims=model_params["glove_large.bin"]['dims'])
    wmv_2 = VectorModelWrap("glove.840B.300d.txt", glove=model_params["glove.840B.300d.txt"]['glove'],
                            binary=model_params["glove.840B.300d.txt"]['binary'],
                            dims=model_params["glove.840B.300d.txt"]['dims'])

    i = 0
    for word in wmv_2._model.dictionary:
        v_1 = wmv_1.get_word_vector(word)
        v_2 = wmv_2.get_word_vector(word)

        print np.mean(v_1), np.mean(v_2)

        \'''
        print cosine(v_1, v_2 / np.sqrt(np.sum(np.power(v_2, 2)))),\
            euclidean(v_1, v_2 / np.sqrt(np.sum(np.power(v_2, 2))))
        \'''

        i += 1

        if i == 1000:
            break
    '''
